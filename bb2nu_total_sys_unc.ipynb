{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import factorial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load in dataframe:\n",
    "'''\n",
    "\n",
    "fnameBG = './files/BG_NEW_1d.h5'\n",
    "df_BG_particles = pd.read_hdf(fnameBG, 'Particles')\n",
    "\n",
    "fname = './files/Xe2nu_NEW_1d_brem.h5'\n",
    "df_particles = pd.read_hdf(fname, 'Particles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make fiducial and S2 cuts for all data:\n",
    "'''\n",
    "\n",
    "df_BG_particles = df_BG_particles[(df_BG_particles['nS2'] == 1)&(df_BG_particles['reco_z_min'] > 20)&(df_BG_particles['reco_z_max'] < 510)&(df_BG_particles['reco_r_max'] < 178)]\n",
    "\n",
    "df_particles = df_particles[(df_particles['nS2'] == 1)&(df_particles['reco_z_min'] > 20)&(df_particles['reco_z_max'] < 510)&(df_particles['reco_r_max'] < 178)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make remaining analysis cuts here:\n",
    "'''\n",
    "\n",
    "df_BG_particles_selection = df_BG_particles[(df_BG_particles['reco_numb_of_tracks'] == 1)&(df_BG_particles['reco_ovlp_blob_energy'] == 0.0)\n",
    "                                      &(df_BG_particles['reco_eblob2']*1000 > 355.55*(1-np.exp(-0.00133*df_BG_particles['reco_energy']*1000)))]\n",
    "\n",
    "df_particles_selection = df_particles[(df_particles['reco_numb_of_tracks'] == 1)&(df_particles['reco_ovlp_blob_energy'] == 0.0)\n",
    "                                      &(df_particles['reco_eblob2']*1000 > 355.55*(1-np.exp(-0.00133*df_particles['reco_energy']*1000)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eplot\n",
    "<br>\n",
    "The following cell is a function called \"eplot\" and is used to plot the reconstructed energy whether it be selected or not selected with whatever set of weights necessary. To use it you must define whether you want to plot the selected reconstructed energy or total reconstructed energy in the reco_energy spot, and then input the weight vector to pair with it, then the sample size variable used, and finally a string that names the physics variable being studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eplot(reco_energy, weight, sample_size, name):\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2, figsize=(10,10), sharex=True, gridspec_kw={'height_ratios': [1, 1]})\n",
    "\n",
    "    ebins = np.linspace(1,2.3,100)\n",
    "    bin_centers = (ebins[1:] + ebins[:-1])/2\n",
    "    energy_counts,_ = np.histogram(reco_energy, bins=ebins)\n",
    "\n",
    "    #Creating matrices for the total and average histogram counts\n",
    "    energy_counts_reweight_all = np.zeros((sample_size,len(energy_counts)))\n",
    "    energy_counts_reweight = np.zeros(len(energy_counts))\n",
    "\n",
    "    #For loop that stores the histogram values for the counts into the previously created matrices\n",
    "    for i in range(0,sample_size):\n",
    "        energy_counts_reweight_hist,_ = np.histogram(reco_energy, bins=ebins, weights=weight[i])\n",
    "        energy_counts_reweight_all[i] = energy_counts_reweight_hist\n",
    "\n",
    "    #Calcluates the average of all the values and stores that into the energy_counts_reweight matrix\n",
    "    energy_counts_reweight = energy_counts_reweight_all.mean(0)\n",
    "    #Calculates the standard deviation of the calculated values and the sqrt of the counts for the total error\n",
    "    yerr_tot = energy_counts_reweight_all.std(0) + np.sqrt(energy_counts_reweight)\n",
    "\n",
    "    # Reweighted Counts\n",
    "#     ax[0].errorbar(bin_centers, energy_counts_reweight, yerr=yerr_tot, fmt='o', label='Reweighted\\n reconstructed\\n background tracks\\n w/ systematic uncertainty');\n",
    "    #Original counts - Not Reweighted\n",
    "    ax[0].errorbar(bin_centers, energy_counts, yerr=yerr_tot, fmt='.', label='Reconstructed\\n background tracks\\n w/ systematic uncertainty');\n",
    "\n",
    "    ax[1].plot(bin_centers, energy_counts_reweight_all.std(0)/energy_counts_reweight, label='Systematic')\n",
    "    ax[1].plot(bin_centers, np.sqrt(energy_counts_reweight_all.mean(0))/energy_counts_reweight_all.mean(0), label='Statistical')\n",
    "\n",
    "    ax[1].set_xlabel('Total reconstructed event energy [MeV]', fontsize=16)\n",
    "    ax[0].set_ylabel('Events', fontsize=16)\n",
    "    ax[1].set_ylabel('Fractional \\nUncertainty', fontsize = 16)\n",
    "    ax[0].tick_params(which='major', axis='both', labelsize=16)\n",
    "    ax[0].set_title('Effect of ' + name +'\\n', fontweight = 'bold', fontsize = 25)\n",
    "\n",
    "    ax[0].legend(loc='upper right', fontsize=16)\n",
    "    ax[1].legend(loc='upper left', fontsize=16)\n",
    "\n",
    "    return (energy_counts_reweight_all.std(0)/energy_counts_reweight).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brem Weight Calculation\n",
    "The following calculation uses two stages of for loops to randomly select variables that are used to perform the calculation with, this random sampling should eliminate the uncertainty/bias in the calculation as best as possible. \n",
    "<br>\n",
    "<br>\n",
    "The first loop selects a random systematic uncertainty for the weight calculation from a normal distribution situated around 0 with a width of 0.1, giving basically a random systematic uncertainty in the range of +/- 10%.\n",
    "<br>\n",
    "<br>\n",
    "The first for loop also randomaly selects a width for which the energy bins can be sliced into several chunks of bins. This should eliminate any energy bias in the calculation. \n",
    "<br>\n",
    "<br>\n",
    "The second for loop calculates the weights of the events based on the radomly selected systematic uncertainty and runs through the loop however many times is necessary to calculate the weight of every bin using the slice width that was selected.\n",
    "<br>\n",
    "To calculate the weights, we take the ratio of Poisson's with the new rate to the old rate:\n",
    "<br><br>\n",
    "$w_k = \\frac{f_1(k)}{f_0(k)} = e^{-(\\lambda_1 - \\lambda_0)}(\\frac{\\lambda_1}{\\lambda_0})^k$.\n",
    "<br><br>\n",
    "where $\\lambda_1 = \\lambda_0(1 \\pm Systematic Uncertainty)$ \n",
    "<br>\n",
    "<br>\n",
    "The first for loop for selecting the sample size can be run as many times as is necessary, however a sample size of around 2800 to 3000 is where the ending values tend to be very steady. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "sample_size = 3000\n",
    "\n",
    "def brem_weight(particles):\n",
    "    \n",
    "    sample_size = 3000\n",
    "    \n",
    "    brem_weight = np.zeros((sample_size,len(particles['n_brems'])))\n",
    "\n",
    "    #Begin for loop that calculates the weights for each randomly selected uncertainty from 1% - 10% and randomly selected bin width from .2 to 2\n",
    "    for j in range(0,sample_size):\n",
    "        syst_unc = np.random.normal(0, 0.1)\n",
    "        eslice_width = np.random.uniform()*(2-.2)+.2\n",
    "        ebins = np.arange(0, 4.0, eslice_width)\n",
    "        ebins[-1] = 100\n",
    "        for i in range(len(ebins)-1):\n",
    "            eslice = (particles['reco_energy'] >= ebins[i])&(particles['reco_energy'] < ebins[i+1])\n",
    "            rate0 = particles[eslice]['n_brems'].mean()\n",
    "            rate1 = rate0*(1 + syst_unc)\n",
    "            if rate0 != 0:\n",
    "                particles.loc[eslice, 'weight'] = np.exp(-(rate1-rate0))*(rate1/rate0)**particles[eslice]['n_brems']\n",
    "            if rate0 == 0:\n",
    "                particles.loc[eslice, 'weight'] = 0**particles[eslice]['n_brems']\n",
    "        #stores the weights for an uncertainty\n",
    "        brem_weight[j] = particles['weight']\n",
    "   \n",
    "    return brem_weight\n",
    "\n",
    "grouped_particles_selection = df_particles_selection.groupby('event').first()\n",
    "\n",
    "t0 = time.clock()\n",
    "\n",
    "weight_all_BG = brem_weight(df_BG_particles)\n",
    "weight_all_selection_BG = brem_weight(df_BG_particles_selection)\n",
    "\n",
    "weight_all = brem_weight(df_particles)\n",
    "weight_all_selection = brem_weight(df_particles_selection)\n",
    "weight_all_selection_g = brem_weight(grouped_particles_selection)\n",
    "\n",
    "t1 = time.clock()-t0\n",
    "print('Time Elapsed:', t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brem_weight_ave_BG = eplot(df_BG_particles['reco_energy'], weight_all_BG, sample_size, 'brems (All)')\n",
    "brem_weight_ave_selected_BG = eplot(df_BG_particles_selection['reco_energy'], weight_all_selection_BG, sample_size, 'brems (Selected)')\n",
    "\n",
    "brem_weight_ave = eplot(df_particles['reco_energy'], weight_all, sample_size, 'brems (All)')\n",
    "brem_weight_ave_selected = eplot(df_particles_selection['reco_energy'], weight_all_selection, sample_size, 'brems (Selected)')\n",
    "brem_weight_ave_selected_g = eplot(grouped_particles_selection['reco_energy'], weight_all_selection_g, sample_size, 'grouped brems (Selected)')\n",
    "\n",
    "print('Average Background Uncertainty:' + str(brem_weight_ave_BG))\n",
    "print('Average Selected Background Uncertainty:' + str(brem_weight_ave_selected_BG))\n",
    "\n",
    "print('Average Uncertainty:' + str(brem_weight_ave))\n",
    "print('Average Selected Uncertainty:' + str(brem_weight_ave_selected))\n",
    "print('Average Selected Grouped Uncertainty:' + str(brem_weight_ave_selected_g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mulitple Scattering Angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Coulomb Scattering Weight Calculation\n",
    "<br>\n",
    "The cell below calculates the weights by shifting the multiple scattering angle up or down by some fraction (syst_unc). The weight is calculated by removing that fraction of events:\n",
    "<br><br>\n",
    "$ 1.0*(1-sys\\_unc)$\n",
    "<br><br>\n",
    "and adding it to the adjacent bin:\n",
    "<br><br>\n",
    "$\\frac{mcs\\_counts[i\\pm1]}{mcs\\_counts[i]}*sys\\_unc$\n",
    "<br><br>\n",
    "If the sys_unc is positive the fraction of events are added to the [ i-1 ] bin and if the sys_unc is negative the fraction of events is added to the [ i+1 ] bin.\n",
    "<br>\n",
    "<br>\n",
    "To calculate the weight due to the multiple scattering angle an overall uncertainty of 10% was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_particles['mcs_min'] = df_particles.groupby('event')['mcs_angle'].transform('min')\n",
    "df_particles['mcs_max'] = df_particles.groupby('event')['mcs_angle'].transform('max')\n",
    "df_particles_selection['mcs_min'] = df_particles_selection.groupby('event')['mcs_angle'].transform('min')\n",
    "df_particles_selection['mcs_max'] = df_particles_selection.groupby('event')['mcs_angle'].transform('max')\n",
    "\n",
    "sample_size = 3000\n",
    "\n",
    "def mcs_weight(particles, angle):\n",
    "    \n",
    "    sample_size = 3000\n",
    "    \n",
    "    weight_mcs = np.zeros((sample_size, len(particles[angle])))\n",
    "\n",
    "    for j in range(0,sample_size):\n",
    "        syst_unc = np.random.normal(0, 0.1)\n",
    "        bin_width = np.random.uniform()*(2-.1)+.1\n",
    "\n",
    "        mcsbins = np.arange(0, 3.15, bin_width)\n",
    "\n",
    "        particles['weight_mcs'] = 1.\n",
    "\n",
    "        mcs_counts,_ = np.histogram(particles[angle], bins=mcsbins)\n",
    "        weight_vec = np.ones(len(mcsbins) - 1)\n",
    "\n",
    "        for i in range(len(mcsbins) - 1):\n",
    "            bin_slice = (particles[angle] >= mcsbins[i])&(particles[angle] < mcsbins[i+1])\n",
    "            if syst_unc > 0 and i != 0:\n",
    "                particles.loc[bin_slice, 'weight_mcs'] = (1-syst_unc) + mcs_counts[i-1]/mcs_counts[i]*syst_unc\n",
    "                weight_vec[i] = (1-syst_unc) + mcs_counts[i-1]/mcs_counts[i]*syst_unc\n",
    "            elif syst_unc <= 0 and i+1 != len(mcs_counts):\n",
    "                particles.loc[bin_slice, 'weight_mcs'] = (1-syst_unc) + mcs_counts[i+1]/mcs_counts[i]*syst_unc\n",
    "                weight_vec[i] = (1-syst_unc) + mcs_counts[i+1]/mcs_counts[i]*syst_unc\n",
    "\n",
    "        weight_norm = len(particles)/particles['weight_mcs'].sum()\n",
    "\n",
    "        particles['weight_mcs'] = particles['weight_mcs']*weight_norm\n",
    "\n",
    "        weight_mcs[j] = particles['weight_mcs']\n",
    "        \n",
    "    return weight_mcs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_mcs_all_BG = mcs_weight(df_BG_particles, 'mcs_angle')\n",
    "weight_mcs_selection_BG = mcs_weight(df_BG_particles_selection, 'mcs_angle')\n",
    "weight_mcs_all = mcs_weight(df_particles, 'mcs_angle')\n",
    "weight_mcs_selection = mcs_weight(df_particles_selection, 'mcs_angle')\n",
    "\n",
    "mcs_weight_ave_BG = eplot(df_BG_particles['reco_energy'], weight_mcs_all_BG, sample_size, 'mcs_angle-No Groups-BG (All)')\n",
    "mcs_weight_ave_selected_BG = eplot(df_BG_particles_selection['reco_energy'], weight_mcs_selection_BG, sample_size, 'mcs_angle-No Groups-BG (Selected)')\n",
    "mcs_weight_ave = eplot(df_particles['reco_energy'], weight_mcs_all, sample_size, 'mcs_angle-No Groups (All)')\n",
    "mcs_weight_ave_selected = eplot(df_particles_selection['reco_energy'], weight_mcs_selection, sample_size, 'mcs_angle-No Groups (Selected)')\n",
    "\n",
    "print('Average Background Uncertainty:' + str(mcs_weight_ave_BG))\n",
    "print('Average Selected Background Uncertainty:' + str(mcs_weight_ave_selected_BG))\n",
    "print('Average Uncertainty:' + str(mcs_weight_ave))\n",
    "print('Average Selected Uncertainty:' + str(mcs_weight_ave_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_mcs_all_g1 = mcs_weight(df_particles.groupby('event').first(), 'mcs_min')\n",
    "weight_mcs_all_g2 = mcs_weight(df_particles.groupby('event').first(), 'mcs_max')\n",
    "\n",
    "mcs_weight_ave_g1 = eplot(df_particles.groupby('event')['reco_energy'].first(), weight_mcs_all_g1, sample_size, 'mcs_angle-Min Angle (All)')\n",
    "mcs_weight_ave_g2 = eplot(df_particles.groupby('event')['reco_energy'].first(), weight_mcs_all_g2, sample_size, 'mcs_angle-Max Angle (All)')\n",
    "\n",
    "print('Average Uncertainty Min Angle:' + str(mcs_weight_ave_g1))\n",
    "print('Average Uncertainty Max Angle:' + str(mcs_weight_ave_g2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_mcs_selection_g1 = mcs_weight(df_particles_selection.groupby('event').first(), 'mcs_min')\n",
    "weight_mcs_selection_g2 = mcs_weight(df_particles_selection.groupby('event').first(), 'mcs_max')\n",
    "\n",
    "mcs_weight_ave_selected_g1 = eplot(df_particles_selection.groupby('event')['reco_energy'].first(), weight_mcs_selection_g1, sample_size, 'mcs_angle-Min Angle (Selected)')\n",
    "mcs_weight_ave_selected_g2 = eplot(df_particles_selection.groupby('event')['reco_energy'].first(), weight_mcs_selection_g2, sample_size, 'mcs_angle-Max Angle (Selected)')\n",
    "\n",
    "print('Average Uncertainty Selected Min Angle:' + str(mcs_weight_ave_selected_g1))\n",
    "print('Average Uncertainty Selected Max Angle:' + str(mcs_weight_ave_selected_g2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Systematic Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_unc_ave_BG_selected = np.sqrt(brem_weight_ave_selected_BG**2+mcs_weight_ave_selected_BG**2)\n",
    "\n",
    "total_unc_ave_selected = np.sqrt(brem_weight_ave_selected_g**2+mcs_weight_ave_selected_g1**2+mcs_weight_ave_selected_g2**2)\n",
    "\n",
    "\n",
    "print('Total Backgroung Sys. Unc. = ' + str(total_unc_ave_BG_selected))\n",
    "print('Total Signal Sys. Unc. = ' + str(total_unc_ave_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
